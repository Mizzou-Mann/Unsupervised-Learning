% assignment_1.tex
% CS 8735 - Unsupervised Learning (Fall 2015)
%     University of Missouri-Columbia
%             Chanmann Lim
%            September 2015

\documentclass[a4paper]{article}

\usepackage[margin=1 in]{geometry}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}

\everymath{\displaystyle}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\begin{document}
\title{CS 8735: Report for assignment 1}
\author{Chanmann Lim}
\date{September 17, 2015}
\maketitle

\noindent The Matlab code for all experiments is in the \textbf{Appendix} section.

\paragraph{Problem 1.} In this task, we are given a dataset generated from a mixture density and the job is to implement EM algorithm to learn the parameters of the model. Based on the assumption that the Gaussian Mixture Model has four component Gaussian PDFs with each having a full covariance matrix we will terminate the our EM estimation at the 100\textsuperscript{th} iterations. \\

\paragraph{a) } For the first experiment which we named it case \textbf{a}, we run EM procedure with the initialization suggested in the assignment.

$$ \pi_k^{(0)} = 1/4 \qquad 1 \le k \le 4 $$
$$ \mu_1^{(0)} = [10\;2]^T, \mu_2^{(0)} = [5\;6]^T, \mu_3^{(0)} = [0\;1]^T, \mu_4^{(0)} = [4\;3]^T $$
$$ \Sigma_k^{(0)} = \mathbf{I}_{2\times2} \qquad 1 \le k \le 4 $$

After the EM procedure terminated, we got

\begin{equation}
	\hat{\pi}_1 = 0.3459, \hat{\pi}_2 = 0.1412, \hat{\pi}_3 = 0.1850, \hat{\pi}_4 = 0.3280\\
\end{equation}
\begin{align}
	\mathbf{\hat{U}} &= \begin{bmatrix}
							\hat{\mu}_1 & \hat{\mu}_2 & \hat{\mu}_3 & \hat{\mu}_4 
						\end{bmatrix} \\
					 & = \begin{bmatrix}
							13.0253  &  4.0666  &  1.6031  &  6.9285 \\
						    3.0467   & 7.9557  &  1.5747  &  5.9848
						 \end{bmatrix}
\end{align}
\begin{align}
	\mathbf{\hat{\Sigma}} &= \begin{bmatrix}
								\hat{\Sigma}_1 & \hat{\Sigma}_2 & \hat{\Sigma}_3 & \hat{\Sigma}_4 
							 \end{bmatrix} \\
						  & = \begin{bmatrix}
								    1.6491  &  0.8845  &  8.4402  &  6.2493 \\
								   -0.7494  &  0.2297  & -0.0623  &  2.6326 \\
								    2.0717  &  1.1715  &  1.0987  &  1.9615
						 	  \end{bmatrix}
\end{align}

Where, $\hat{\Sigma_k}$ is the upper triangular values for covariance matrix of the $k^{th}$ Gaussian component. $$1 \le k \le 4$$

Figure ~\ref{fig:log_likelihood_a} shows that EM has converged after the 97\textsuperscript{th} iteration.

\begin{figure}[H]
  \centering
    \includegraphics[scale=.57]{images/log_likelihood_scores.png}
  \caption{Log likelihood scores for case \textbf{a}}
  \label{fig:log_likelihood_a}
\end{figure}

To see the effect of EM algorithm visually we assign each data point to one of the four clusters $k = 1,2,3,4$ using the maximum posterior probability rule then plot three separate graphs for $t = 10,50,100$.

$$ k^* = \argmax_{1 \le k \le 4} P(z_n=k|x_n;\Theta^{(t)}) $$

\begin{figure}[H]
  \centering
    \includegraphics[scale=.57]{images/clusters_t_10.png}
  \caption{Plot of the four clusters at t=10}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[scale=.57]{images/clusters_t_50.png}
  \caption{Plot of the four clusters at t=50}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[scale=.57]{images/clusters_t_100.png}
  \caption{Plot of the four clusters at t=100}
\end{figure}

\paragraph{b) } For the second experiment(case \textbf{b}) with the same dataset we are going to use a different initialization for the parameters $\Theta^{(0)} = \{ \pi^{(0)}, \mu^{(0)}, \Sigma^{(0)} \}$ under the same assumption that the data comes from	four components gaussian mixture model and EM procedure will converge at the 100\textsuperscript{th} iterations.\\

The plot of the data will actually help reveal its natural grouping to some extent before our blind guess and this is especially true for two dimensional dataset like in this problem.

\begin{figure}[H]
  \centering
    \includegraphics[scale=.57]{images/GMD_dat.png}
  \caption{Plot of GMD.dat}
  \label{fig:GMD.dat}
\end{figure}

And from Figure ~\ref{fig:GMD.dat} we comes up with $\Theta^{(0)}$ as the following:

$$ \pi_1^{(0)} = 0.25, \pi_2^{(0)} = 0.2, \pi_3^{(0)} = 0.25, \pi_4^{(0)} = 0.3 $$
$$ \mu_1^{(0)} = [1\quad2]^T, \mu_2^{(0)} = [4\quad8]^T, \mu_3^{(0)} = [8\quad6.5]^T, \mu_4^{(0)} = [13.5\quad3]^T $$
$$ \Sigma_k^{(0)} = \mathbf{I}_{2\times2} \qquad 1 \le k \le 4 $$

Empirically we can select several points closed to each already chosen $\mu_k^{(0)}$ at random to compute for the covariance matrix $\Sigma$ however that wouldn't guarantee to give measurable accuracy then any purely random guess covariance matrix than using the same covariance matrix $\Sigma_k^{(0)} = \mathbf{I}_{2\times2}$ as in case \textbf{a} will be as satisfactory.\\

And the EM procedure terminated with

\begin{equation}
	\hat{\pi}_1 = 0.1847, \hat{\pi}_2 = 0.1401, \hat{\pi}_3 = 0.3295, \hat{\pi}_4 = 0.3457\\
\end{equation}
\begin{align}
	\mathbf{\hat{U}} &= \begin{bmatrix}
							\hat{\mu}_1 & \hat{\mu}_2 & \hat{\mu}_3 & \hat{\mu}_4 
						\end{bmatrix} \\
					 & = \begin{bmatrix}
							1.6026  &  4.0619  &  6.9182  & 13.0263\\
						    1.5717  &  7.9675  &  5.9843  &  3.0455
						 \end{bmatrix}
\end{align}
\begin{align}
	\mathbf{\hat{\Sigma}} &= \begin{bmatrix}
								\hat{\Sigma}_1 & \hat{\Sigma}_2 & \hat{\Sigma}_3 & \hat{\Sigma}_4 
							 \end{bmatrix} \\
						  & = \begin{bmatrix}
									8.4468  &  0.8788  &  6.2733  &  1.6470 \\
								   -0.0635  &  0.2342  &  2.6295  & -0.7471 \\
								    1.0938  &  1.1568  &  1.9615  &  2.0688
						 	  \end{bmatrix}
\end{align}

As shown in Figure ~\ref{fig:log_likelihood_b} good initialization will lead to faster convergence. \\

\begin{figure}[H]
  \centering
    \includegraphics[scale=.57]{images/log_likelihood_scores_b.png}
  \caption{Log likelihood scores for case \textbf{b}}
  \label{fig:log_likelihood_b}
\end{figure}

\paragraph{Problem 2.} From the coin-tossing example discussed in class we know that there are two coins(A and B) equally likely to be selected at random to perform the tossing experiment which produced the following outcome. \\

\begin{center}
  \begin{tabular}{  r | c | c }
                 & Coin A & Coin B \\ \hline
    $\mathbf{x_1}$ &        & 5H, 5T \\ 
    $\mathbf{x_2}$ & 9H, 1T &        \\ 
    $\mathbf{x_3}$ & 8H, 2T &        \\ 
    $\mathbf{x_4}$ &        & 4H, 6T \\ 
    $\mathbf{x_5}$ & 7H, 3T &        
  \end{tabular}
\end{center}

And

$$ P(z = A) = P(z = B) = 0.5 $$
$$ \theta_A = P(H|z=A), \quad \theta_B = P(H|z=B)  $$

For this task we begin with $\theta^{(0)} = (\theta^{(0)}_A, \theta^{(0)}_B) = (0.6, 0.4)$ and terminate the EM procedure at the 10\textsuperscript{th} iterations. We obtain the estimate of the parameters for $t = 1,2,\cdots,10$ as the following: \\

\begin{tabular}{ | r | c c c c c c c c c c | }
	\hline
	t & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
	$\theta_A^{(t)}$ & 0.7261  &  0.7680  &  0.7852  &  0.7923  &  0.7951  &  0.7961  &  0.7965  &  0.7967  &  0.7968  &  0.7968 \\
	$\theta_B^{(t)}$ & 0.5020  &  0.5194  &  0.5208  &  0.5203  &  0.5199  &  0.5197  &  0.5196  &  0.5196  &  0.5196  &  0.5196 \\ \hline
\end{tabular}

~\\

Now with the estimated parameters $\theta_A^{(t)}$ and $\theta_B^{(t)}$ we can compute the posterior probability $P(z^n = A|x^n; \theta^{(t)})$ and $P(z^n = B|x^n; \theta^{(t)})$ using Bayes rule:

\begin{equation}
	P(z^n = A|x^n; \theta^{(t)}) = \frac{P(z^n = A) \cdot P(x^n|z^n = A; \theta^{(t)})}{P(x^n; \theta^{(t)})}
\end{equation}

And $P(x^n; \theta^{(t)})$ can be obtained by marginalizing over $z^n$ using the sum rule: 

\begin{equation}
	P(x^n; \theta^{(t)}) = P(z^n = A) \cdot P(x^n|z^n = A; \theta^{(t)}) + P(z^n = B) \cdot P(x^n|z^n = B; \theta^{(t)})
\end{equation}

\begin{center}
\begin{tabular}{ | c | c | c | c | }
	\hline
	& $x^n$ & $P(z^n=A|x^n; \theta^{(t)})$ & $P(z^n=B|x^n; \theta^{(t)})$ \\ \hline
	\multirow{5}{*}{t=1} & 5H, 5T & 0.2416 & 0.7584 \\
		& 9H, 1T & 0.9384 & 0.0616 \\
		& 8H, 2T & 0.8528 & 0.1472 \\
		& 4H, 6T & 0.1080 & 0.8920 \\
		& 7H, 3T & 0.6878 & 0.3122 \\ \hline
    \multirow{5}{*}{t=10} & 5H, 5T & 0.1030 & 0.8970 \\
    	& 9H, 1T & 0.9520 & 0.0480 \\
    	& 8H, 2T & 0.8455 & 0.1545 \\
    	& 4H, 6T & 0.0307 & 0.9693 \\
    	& 7H, 3T & 0.6015 & 0.3985 \\ \hline
\end{tabular}
\end{center}

We get 

\begin{equation}
	\log P(X; \theta^{(t)}) = \sum_{n=1}^5 \log P(x^n; \theta^{(t)})
\end{equation}

The plot of $\log P(X; \theta^{(t)})$ is shown in Figure ~\ref{fig:log_P_X}.

\begin{figure}[H]
  \centering
    \includegraphics[scale=.57]{images/log_P_X.png}
  \caption{Plot of $\log P(X; \theta^{(t)})$}
  \label{fig:log_P_X}
\end{figure}

\paragraph{Problem 3.} Derive the EM estimation for two-dimensional Gaussian Mixture Density parameters $\hat{\pi}_k, \hat{\mu}_k, \hat{\Sigma}_k$ for $k = 1,2,\cdots,K$ given that $\Sigma_1 = \Sigma_2 = \cdots = \Sigma_K = \Sigma$. In EM algorithm we are maximizing $Q(\Theta, \Theta') = \mathbf{E} [ \log P(\mathbf{X}, Z; \Theta) | \mathbf{X}, \Theta']$ with respect to $\pi_k, \mu_k, \text{ and } \Sigma_k$ and according to the discussion in class and the constraints given here we get:

\begin{equation}
	Q(\Theta, \Theta') = \sum_{n=1}^N \sum_{k=1}^K \gamma_{n,k}' \left( \log \pi_k - \log 2\pi - \frac{1}{2}\log |\Sigma| - \frac{1}{2}(x_n-\mu_k)^T\Sigma^{-1}(x_n-\mu_k) \right)
\end{equation}

Where $\gamma_{n,k}' = P(z_n=k|x_n; \Theta')$

\begin{equation}
	\gamma_{n,k}' = \frac{\pi_k'N(x_n;\mu_k', \Sigma')}{\sum_{j=1}^K \pi_k'N(x_n;\mu_k', \Sigma')}
\end{equation}	

Then

\begin{align}
	Q_{\mu} &= -\frac{1}{2} \sum_{n=1}^N \sum_{k=1}^K \gamma_{n,k}' (x_n-\mu_k)^T\Sigma^{-1}(x_n-\mu_k) \\
	\frac{\partial Q_{\mu}}{\partial \mu_k} &= \Sigma^{-1} \sum_{n=1}^N \gamma_{n,k}' (x_n-\mu_k)\\
	\sum_{n=1}^N \gamma_{n,k}' x_n - \sum_{n=1}^N \gamma_{n,k}' \hat{\mu}_k &= 0 \\
	\hat{\mu}_k &= \frac{1}{\sum_{n=1}^N \gamma_{n,k}'} \sum_{n=1}^N \gamma_{n,k}' x_n
\end{align}

And

\begin{align}
	Q_{\Sigma} &= -\frac{1}{2} \sum_{n=1}^N \sum_{k=1}^K \gamma_{n,k}' \left(\log |\Sigma| + (x_n-\mu_k)^T\Sigma^{-1}(x_n-\mu_k) \right) \\
		&= -\frac{1}{2}\left(\log |\Sigma| \sum_{n=1}^N \sum_{k=1}^K \gamma_{n,k}' + \sum_{n=1}^N \sum_{k=1}^K \gamma_{n,k}' (x_n-\mu_k)^T\Sigma^{-1}(x_n-\mu_k) \right) \\
		&= -\frac{1}{2}\left(N\log |\Sigma| + \sum_{n=1}^N \sum_{k=1}^K \gamma_{n,k}' (x_n-\mu_k)^T\Sigma^{-1}(x_n-\mu_k) \right) \\
		\frac{\partial Q_{\Sigma}}{\partial \Sigma^{-1}} &= -\frac{1}{2}\left( -N\Sigma + \sum_{n=1}^N (x_n-\mu_k)(x_n-\mu_k)^T \right) \\
		-N\hat{\Sigma} + \sum_{n=1}^N (x_n-\mu_k)(x_n-\mu_k)^T &= 0 \\
		\hat{\Sigma} &= \frac{1}{N} \sum_{n=1}^N (x_n-\mu_k)(x_n-\mu_k)^T
\end{align}

And

\begin{align}
	Q_{\pi} &= \sum_{n=1}^N \sum_{k=1}^K \gamma_{n,k}' \log \pi_k + \lambda(\sum_{k=1}^K \pi_k-1) \\
	\frac{\partial Q_{\pi}}{\partial \pi_k} &= \sum_{n=1}^N \gamma_{n,k}' \frac{1}{\pi_k} + \lambda \\
	\sum_{n=1}^N \gamma_{n,k}' + \lambda \hat{\pi}_k &= 0, \quad \sum_{n=1}^N \sum_{k=1}^K \gamma_{n,k}' + \lambda \sum_{k=1}^K \pi_k = 0 \\
	\hat{\pi}_k &= \frac{1}{N} \sum_{n=1}^N \gamma_{n,k}'
\end{align}



\newpage
\subsection*{Appendix:}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{assignment_1.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{problem_1_a.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{problem_1_b.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{EM.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{gamma_nk.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{mvnpdf.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{log_P.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{sigma_d.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{vectorize_sigma.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{classify.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{clusters_plot.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{problem_2.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{EM_2.m}
	\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{bernoulli.m}
\end{document}